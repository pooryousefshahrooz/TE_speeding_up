{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from absl import app\n",
    "from absl import flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from env import Environment\n",
    "from game import DRL_Game\n",
    "from model import Network\n",
    "from config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('num_agents', 20, 'number of agents')\n",
    "flags.DEFINE_string('baseline', 'avg', 'avg: use average reward as baseline, best: best reward as baseline')\n",
    "flags.DEFINE_integer('num_iter', 10, 'Number of iterations each agent would run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENTS_CHECK=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_agent(config, game, model_weights_queues, experience_queues):\n",
    "    network = Network(config, game.state_dims, game.action_dim, game.max_moves, master=True)\n",
    "    network.save_hyperparams(config)\n",
    "    start_step = network.restore_ckpt()\n",
    "    for step in tqdm(range(start_step, config.max_step), ncols=70, initial=start_step):\n",
    "        network.ckpt.step.assign_add(1)\n",
    "        model_weights = network.model.get_weights()\n",
    "        for i in range(FLAGS.num_agents):\n",
    "            model_weights_queues[i].put(model_weights)\n",
    "        if config.method == 'actor_critic':\n",
    "            #assemble experiences from the agents\n",
    "            s_batch = []\n",
    "            a_batch = []\n",
    "            r_batch = []\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                s_batch_agent, a_batch_agent, r_batch_agent = experience_queues[i].get()\n",
    "              \n",
    "                assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent))\n",
    "                s_batch += s_batch_agent\n",
    "                a_batch += a_batch_agent\n",
    "                r_batch += r_batch_agent\n",
    "           \n",
    "            assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "            #used shared RMSProp, i.e., shared g\n",
    "            actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "            value_loss, entropy, actor_gradients, critic_gradients = network.actor_critic_train(np.array(s_batch), \n",
    "                                                                    actions, \n",
    "                                                                    np.array(r_batch).astype(np.float32), \n",
    "                                                                    config.entropy_weight)\n",
    "       \n",
    "            if GRADIENTS_CHECK:\n",
    "                for g in range(len(actor_gradients)):\n",
    "                    assert np.any(np.isnan(actor_gradients[g])) == False, ('actor_gradients', s_batch, a_batch, r_batch, entropy)\n",
    "                for g in range(len(critic_gradients)):\n",
    "                    assert np.any(np.isnan(critic_gradients[g])) == False, ('critic_gradients', s_batch, a_batch, r_batch)\n",
    "            if step % config.save_step == config.save_step - 1:\n",
    "                network.save_ckpt(_print=True)\n",
    "                \n",
    "                #log training information\n",
    "                actor_learning_rate = network.lr_schedule(network.actor_optimizer.iterations.numpy()).numpy()\n",
    "                avg_value_loss = np.mean(value_loss)\n",
    "                avg_reward = np.mean(r_batch)\n",
    "                avg_entropy = np.mean(entropy)\n",
    "            \n",
    "                network.inject_summaries({\n",
    "                    'learning rate': actor_learning_rate,\n",
    "                    'value loss': avg_value_loss,\n",
    "                    'avg reward': avg_reward,\n",
    "                    'avg entropy': avg_entropy\n",
    "                    }, step)\n",
    "                print('lr:%f, value loss:%f, avg reward:%f, avg entropy:%f'%(actor_learning_rate, avg_value_loss, avg_reward, avg_entropy))\n",
    "        elif config.method == 'pure_policy':\n",
    "            #assemble experiences from the agents\n",
    "            s_batch = []\n",
    "            a_batch = []\n",
    "            r_batch = []\n",
    "            ad_batch = []\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                s_batch_agent, a_batch_agent, r_batch_agent, ad_batch_agent = experience_queues[i].get()\n",
    "              \n",
    "                assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent), len(ad_batch_agent))\n",
    "                s_batch += s_batch_agent\n",
    "                a_batch += a_batch_agent\n",
    "                r_batch += r_batch_agent\n",
    "                ad_batch += ad_batch_agent\n",
    "           \n",
    "            assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "            #used shared RMSProp, i.e., shared g\n",
    "            actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "            entropy, gradients = network.policy_train(np.array(s_batch), \n",
    "                                                      actions, \n",
    "                                                      np.vstack(ad_batch).astype(np.float32), \n",
    "                                                      config.entropy_weight)\n",
    "            if GRADIENTS_CHECK:\n",
    "                for g in range(len(gradients)):\n",
    "                    assert np.any(np.isnan(gradients[g])) == False, (s_batch, a_batch, r_batch)\n",
    "            \n",
    "            if step % config.save_step == config.save_step - 1:\n",
    "                network.save_ckpt(_print=True)\n",
    "                \n",
    "                #log training information\n",
    "                learning_rate = network.lr_schedule(network.optimizer.iterations.numpy()).numpy()\n",
    "                avg_reward = np.mean(r_batch)\n",
    "                avg_advantage = np.mean(ad_batch)\n",
    "                avg_entropy = np.mean(entropy)\n",
    "                network.inject_summaries({\n",
    "                    'learning rate': learning_rate,\n",
    "                    'avg reward': avg_reward,\n",
    "                    'avg advantage': avg_advantage,\n",
    "                    'avg entropy': avg_entropy\n",
    "                    }, step)\n",
    "                print('lr:%f, avg reward:%f, avg advantage:%f, avg entropy:%f'%(learning_rate, avg_reward, avg_advantage, avg_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(agent_id, config, game, tm_subset, model_weights_queue, experience_queue):\n",
    "    scenarios = game.lp_links\n",
    "    purified_scenarios = []\n",
    "    for scenario in scenarios:\n",
    "        if scenario not in purified_scenarios and [(scenario[1],scenario[0])] not in purified_scenarios:\n",
    "            purified_scenarios.append(scenario)\n",
    "    scales = [1,1.2,1.4,1.8,2.2,2.4,2.6,3,3.4]\n",
    "    \n",
    "    env = Environment(config, is_training=True)\n",
    "    for failed_link in purified_scenarios:\n",
    "        for scale in scales:\n",
    "            random_state = np.random.RandomState(seed=agent_id)\n",
    "            network = Network(config, game.state_dims, game.action_dim, game.max_moves, master=False)\n",
    "\n",
    "            # initial synchronization of the model weights from the coordinator \n",
    "            model_weights = model_weights_queue.get()\n",
    "            network.model.set_weights(model_weights)\n",
    "            idx = 0\n",
    "            s_batch = []\n",
    "            a_batch = []\n",
    "            r_batch = []\n",
    "            if config.method == 'pure_policy':\n",
    "                ad_batch = []\n",
    "            run_iteration_idx = 0\n",
    "            num_tms = len(tm_subset)\n",
    "            random_state.shuffle(tm_subset)\n",
    "            run_iterations = FLAGS.num_iter\n",
    "\n",
    "            while True:\n",
    "                tm_idx = tm_subset[idx]\n",
    "                #state\n",
    "                state = game.get_state2(env,tm_idx,scale,failed_link)\n",
    "                import pdb\n",
    "\n",
    "                s_batch.append(state)\n",
    "                #action\n",
    "                if config.method == 'actor_critic':    \n",
    "                    policy = network.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "                elif config.method == 'pure_policy':\n",
    "                    policy = network.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "                assert np.count_nonzero(policy) >= game.max_moves, (policy, state)\n",
    "                actions = random_state.choice(game.action_dim, game.max_moves, p=policy, replace=False)\n",
    "                for a in actions:\n",
    "                    a_batch.append(a)\n",
    "\n",
    "                #reward\n",
    "                reward = game.reward2(tm_idx, actions,failed_link,scale)\n",
    "                r_batch.append(reward)\n",
    "                #pdb.set_trace()\n",
    "                if config.method == 'pure_policy':\n",
    "                    #advantage\n",
    "                    if config.baseline == 'avg':\n",
    "                        ad_batch.append(game.advantage(tm_idx, reward))\n",
    "                        game.update_baseline(tm_idx, reward)\n",
    "                    elif config.baseline == 'best':\n",
    "                        best_actions = policy.argsort()[-game.max_moves:]\n",
    "                        best_reward = game.reward2(tm_idx, best_actions)\n",
    "                        ad_batch.append(reward - best_reward)\n",
    "                run_iteration_idx += 1\n",
    "                if run_iteration_idx >= run_iterations:\n",
    "                    # Report experience to the coordinator                          \n",
    "                    if config.method == 'actor_critic':    \n",
    "                        experience_queue.put([s_batch, a_batch, r_batch])\n",
    "                    elif config.method == 'pure_policy':\n",
    "                        experience_queue.put([s_batch, a_batch, r_batch, ad_batch])\n",
    "\n",
    "                    #print('report', agent_id)\n",
    "\n",
    "                    # synchronize the network parameters from the coordinator\n",
    "                    model_weights = model_weights_queue.get()\n",
    "                    network.model.set_weights(model_weights)\n",
    "\n",
    "                    del s_batch[:]\n",
    "                    del a_batch[:]\n",
    "                    del r_batch[:]\n",
    "                    if config.method == 'pure_policy':\n",
    "                        del ad_batch[:]\n",
    "                    run_iteration_idx = 0\n",
    "\n",
    "                # Update idx\n",
    "                idx += 1\n",
    "                if idx == num_tms:\n",
    "                   random_state.shuffle(tm_subset)\n",
    "                   idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    #cpu only\n",
    "    tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    config = get_config(FLAGS) or FLAGS\n",
    "    env = Environment(config, is_training=True)\n",
    "    game = DRL_Game(config, env)\n",
    "    model_weights_queues = []\n",
    "    experience_queues = []\n",
    "    if FLAGS.num_agents == 0 or FLAGS.num_agents >= mp.cpu_count():\n",
    "        FLAGS.num_agents = mp.cpu_count() - 1\n",
    "    print('Agent num: %d, iter num: %d\\n'%(FLAGS.num_agents+1, FLAGS.num_iter))\n",
    "    for _ in range(FLAGS.num_agents):\n",
    "        model_weights_queues.append(mp.Queue(1))\n",
    "        experience_queues.append(mp.Queue(1))\n",
    "    tm_subsets = np.array_split(game.tm_indexes, FLAGS.num_agents)\n",
    "    coordinator = mp.Process(target=central_agent, args=(config, game, model_weights_queues, experience_queues))\n",
    "    coordinator.start()\n",
    "    agents = []\n",
    "    for i in range(FLAGS.num_agents):\n",
    "        agents.append(mp.Process(target=agent, args=(i, config, game, tm_subsets[i], model_weights_queues[i], experience_queues[i])))\n",
    "    for i in range(FLAGS.num_agents):\n",
    "        agents[i].start()\n",
    "    coordinator.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
