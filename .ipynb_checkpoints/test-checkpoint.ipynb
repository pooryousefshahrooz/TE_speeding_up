{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from absl import app\n",
    "from absl import flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from env import Environment\n",
    "from game import DRL_Game\n",
    "from model import Network\n",
    "from config import get_config\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('ckpt', '', 'apply a specific checkpoint')\n",
    "flags.DEFINE_boolean('eval_delay', False, 'evaluate delay or not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(config, network, game,each_scale_scenario_MLU_results):\n",
    "    \n",
    "    scenarios = game.lp_links\n",
    "    print(\"we have %s scenarios\",len(scenarios),scenarios)\n",
    "    purified_scenarios = []\n",
    "    for scenario in scenarios:\n",
    "        if scenario not in purified_scenarios and (scenario[1],scenario[0]) not in purified_scenarios and (scenario[0],scenario[1]) not in purified_scenarios:\n",
    "            purified_scenarios.append(scenario)\n",
    "    scales = [3.8,3.4,3.6,3.9,4,4.1]\n",
    "    print(\"we have %s purified_scenarios\",len(purified_scenarios),purified_scenarios)\n",
    "    env = Environment(config, is_training=True)\n",
    "    print(\"******************** game.tm_indexes \",game.tm_indexes)\n",
    "    DM_counter = 0\n",
    "    #purified_scenarios = [(4,5),(0,2)]\n",
    "    for tm_idx in game.tm_indexes:\n",
    "        if DM_counter <=30:\n",
    "            DM_counter +=1\n",
    "            for scale in scales:\n",
    "                for failed_link in purified_scenarios:\n",
    "                    #for scenario (5, 3) scale 3.4 and  DM index 0  \n",
    "#                     tm_idx = 0\n",
    "#                     scale  = 3.4\n",
    "#                     failed_link = (5, 3)\n",
    "                    print('for scenario %s scale %s and  DM index %s '%(failed_link,scale,tm_idx))\n",
    "                    state = game.get_state2(env,tm_idx,scale,failed_link)\n",
    "                    if config.method == 'actor_critic':\n",
    "                        policy = network.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "                    elif config.method == 'pure_policy':\n",
    "                        policy = network.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "                    actions = policy.argsort()[-game.max_moves:]\n",
    "                    #print('action is ',actions)\n",
    "                    randomly_selected_actions = []\n",
    "                    while(len(randomly_selected_actions)<len(actions)):\n",
    "\n",
    "                        random_flow = random.randint(0,game.action_dim-1)\n",
    "                        if random_flow not in randomly_selected_actions:\n",
    "                            randomly_selected_actions.append(random_flow)\n",
    "                    game.evaluate2(tm_idx, failed_link,scale,each_scale_scenario_MLU_results,randomly_selected_actions,actions, eval_delay=FLAGS.eval_delay) \n",
    "#                 game.evaluate(tm_idx,scale, each_scale_scenario_MLU_results,actions,eval_delay=FLAGS.eval_delay)\n",
    "#     for tm_idx in game.tm_indexes:\n",
    "#         state = game.get_state(tm_idx)\n",
    "#         if config.method == 'actor_critic':\n",
    "#             policy = network.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "#         elif config.method == 'pure_policy':\n",
    "#             policy = network.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "#         actions = policy.argsort()[-game.max_moves:]\n",
    "#         game.evaluate(tm_idx, actions, eval_delay=FLAGS.eval_delay) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    #Using cpu for testing\n",
    "    tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "    config = get_config(FLAGS) or FLAGS\n",
    "    env = Environment(config, is_training=False)\n",
    "    game = DRL_Game(config, env)\n",
    "    network = Network(config, game.state_dims, game.action_dim,game.max_moves)\n",
    "    step = network.restore_ckpt(FLAGS.ckpt)\n",
    "    if config.method == 'actor_critic':\n",
    "        learning_rate = network.lr_schedule(network.actor_optimizer.iterations.numpy()).numpy()\n",
    "    elif config.method == 'pure_policy':\n",
    "        learning_rate = network.lr_schedule(network.optimizer.iterations.numpy()).numpy()\n",
    "    print('\\nstep %d, learning rate: %f\\n'% (step, learning_rate))\n",
    "    each_scale_scenario_MLU_results = 'each_scale_scenario_MLU_results_ATT_third_run.csv' \n",
    "    sim(config, network, game,each_scale_scenario_MLU_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
